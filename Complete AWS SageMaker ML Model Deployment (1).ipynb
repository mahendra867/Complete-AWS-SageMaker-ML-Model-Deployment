{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce810b5",
   "metadata": {},
   "source": [
    "### Importing Important Libraries\n",
    "#### Steps To Be Followed\n",
    "1. Importing necessary Libraries\n",
    "2. Creating S3 bucket\n",
    "3. Mapping train And Test Data in S3\n",
    "4. Mapping The path of the models in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ea5bf5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker # in this ml project iam using the some built in ml algorithims like Xg-Boost that are present in SageMaker and we will download that image container which has Xg-boost inbuilt alogrithim \n",
    "import boto3 # with the help of python from our local enivironment we can also able to read S3 bucket which it is Public by this boto3 algorithim\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri # and the whole thing which is XG-boost ml algorithim can be download by get_image_url \n",
    "from sagemaker.session import s3_input,Session  # if we really want to use the instance w.r.t sagemaker we have create a session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e24ea74",
   "metadata": {},
   "source": [
    "Certainly! Let's break down each line of the code:\n",
    "\n",
    "1. `import sagemaker`: This line imports the `sagemaker` library, which provides the necessary tools and functionalities for working with Amazon SageMaker, a fully managed service for building, training, and deploying machine learning models.\n",
    "\n",
    "2. `import boto3`: This line imports the `boto3` library, which is the official AWS SDK (Software Development Kit) for Python. It allows you to interact with various AWS services, including Amazon SageMaker.\n",
    "\n",
    "3. `from sagemaker.amazon.amazon_estimator import get_image_uri`: This line imports the `get_image_uri` function from the `amazon_estimator` module within the `sagemaker.amazon` package. The `get_image_uri` function is used to retrieve the container image URI (Uniform Resource Identifier) for a specific built-in SageMaker algorithm.\n",
    "\n",
    "4. `from sagemaker.session import s3_input, Session`: This line imports the `s3_input` class and the `Session` class from the `session` module within the `sagemaker.session` package. The `s3_input` class is used to create input channels for training a model using data stored in Amazon S3. The `Session` class represents an active SageMaker session and provides methods for working with SageMaker resources.\n",
    "\n",
    "In summary, these import statements bring in the necessary libraries and modules required for working with SageMaker, including interacting with AWS services, accessing built-in algorithms, and managing sessions and data inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8de103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6bab57e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "bucket_name ='bankapplication126' # Change this varaible name to a unique name for your bucket \n",
    "my_region=boto3.session.Session().region_name # i really want to check my region name becasue based on the region name i may be access different subset folders inside my bucket  set the region of the instance and currently iam working in the us east virgina region and there will be scenario that we need to work on diiferent regions based on servers or based on the response time that we get quickly,so if we are in europe region then we will use Europe region then we can get the quick response time at the services thatwe are tyring to use  \n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8904d266",
   "metadata": {},
   "source": [
    "The code snippet provided sets the variable `bucket_name` to a string value representing the name of an Amazon S3 bucket. It also retrieves the AWS region name using the `boto3` library and prints it.\n",
    "\n",
    "Let's break down the code further:\n",
    "\n",
    "1. `bucket_name = 'bankapplication'`: This line assigns the string value `'bankapplication'` to the variable `bucket_name`. This represents the name of the Amazon S3 bucket that will be used in the code. You can change this value to a unique name of your choice, following the rules for bucket naming conventions in Amazon S3.\n",
    "\n",
    "2. `my_region = boto3.session.Session().region_name`: This line retrieves the AWS region name using the `boto3` library. The `boto3.session.Session()` creates a new session object, and `.region_name` retrieves the name of the current AWS region associated with the session.\n",
    "\n",
    "3. `print(my_region)`: This line prints the value of the `my_region` variable, which represents the AWS region name. The region name is typically a string representing the geographical location of the AWS resources being used.\n",
    "\n",
    "By printing the AWS region name, you can verify which region is currently set in the session. This information can be useful when working with AWS services that are region-specific, such as Amazon S3 or Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "795324dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket created successfully\n"
     ]
    }
   ],
   "source": [
    "# creating bucket name by using code \n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1': \n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16585f5f",
   "metadata": {},
   "source": [
    "Basically iam trying to built my model over here i will save my model inside the S3 bucket and i will do the versioning of the model if the new data comes and then again i have to train my model and again i have put back in my s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "60bab1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://bankapplication126/Xgboost-as-a-built-in-algo/output\n"
     ]
    }
   ],
   "source": [
    "# set an output path where the trained model will be saved\n",
    "prefix='Xgboost-as-a-built-in-algo' # currently iam using the Xg-boost algorithim by using Sagemaker ,so prefix is just another folder which iam giving for my algorithim so the model can save inside my output folder\n",
    "output_path ='s3://{}/{}/output'.format(bucket_name, prefix) # suppose if i want to access my bank application i have to create a path like this  \"s3://\" = it means s3 and bank application and since i want to create my model or whatever my model is trained if i want to store in s3 bucket for that i use {in this it is replace by bucket name}/{it will be replace by prefix}\n",
    "print(output_path)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dff485",
   "metadata": {},
   "source": [
    "- the above output is my output path that basically mean once i train my model it will get saved inside this output folder and every time i retrain my model every time a new version of my model will be store with a proper folder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b048865",
   "metadata": {},
   "source": [
    "### Downloading The Dataset And Storing in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "00aff995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: downloaded bank_clean.csv.\n",
      "Success: Data loaded into dataframe.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "try:\n",
    "    urllib.request.urlretrieve(\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\", \"bank_clean.csv\")\n",
    "    print('Success: downloaded bank_clean.csv.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n",
    "\n",
    "try:\n",
    "    model_data = pd.read_csv('./bank_clean.csv',index_col=0)\n",
    "    print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4b7bc970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41188, 61)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628d86f",
   "metadata": {},
   "source": [
    "Certainly! Let's go through the code step by step to explain what it does:\n",
    "\n",
    "1. The code first imports the necessary libraries. `pandas` is imported as `pd` to provide data manipulation and analysis tools. `urllib` is imported to handle the URL retrieval.\n",
    "\n",
    "2. The code then attempts to download a CSV file from a specified URL using the `urlretrieve` function from `urllib.request`. The URL points to a file named `bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv`. If the download is successful, it saves the file as `bank_clean.csv`.\n",
    "\n",
    "3. Inside a try-except block, the code checks if the download was successful or if any exception occurred during the process. If the download is successful, it prints the message \"Success: downloaded bank_clean.csv.\" If an exception occurs, it prints the error message \"Data load error\" along with the specific error.\n",
    "\n",
    "4. The code then attempts to read the downloaded CSV file into a pandas DataFrame using the `read_csv` function. The `index_col=0` parameter specifies that the first column of the CSV should be used as the index of the DataFrame.\n",
    "\n",
    "5. Inside another try-except block, the code checks if the data loading into the DataFrame was successful or if any exception occurred. If the data is loaded successfully, it prints the message \"Success: Data loaded into dataframe.\" If an exception occurs, it prints the error message \"Data load error\" along with the specific error.\n",
    "\n",
    "Overall, this code downloads a CSV file from a specified URL and reads it into a pandas DataFrame, providing a way to access and work with the data in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f9d28442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 61) (12357, 61)\n"
     ]
    }
   ],
   "source": [
    "# Train test split \n",
    "# we are performing the train_test split to save that model_data in the S3 bucket so that we can be re use any no of time later furthur if we needed\n",
    "import numpy as np\n",
    "train_data,test_data=np.split(model_data.sample(frac=1,random_state=1729),[int(0.7 * len(model_data))])\n",
    "print(train_data.shape,test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0f3cbe",
   "metadata": {},
   "source": [
    "### IMP \n",
    "whenever if we are specifcally dealing with AWS Amazon Sagemaker the dependent column should be our first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "73783081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "### Saving Train And Test Into Buckets\n",
    "## We start with Train Data\n",
    "import os\n",
    "# Save the train data to a CSV file\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)],  axis=1).to_csv('train.csv', index=False, header=False)\n",
    "                                                \n",
    "                                               \n",
    "\n",
    "# Upload the train data to S3 Bucket\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "\n",
    "# whenever we train our model the path data is given to S3 bucket\n",
    "# Create an S3 input channel for the train data\n",
    "s3_input_train = s3_input(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ac11f",
   "metadata": {},
   "source": [
    "Certainly! Let's go through each part of the code snippet step by step:\n",
    "\n",
    "1. `pd.concat([...], axis=1).to_csv('train.csv', index=False, header=False)`: This line concatenates the `'y_yes'` column from the `train_data` DataFrame with the remaining columns (excluding `'y_no'` and `'y_yes'`) using `pd.concat()`. The resulting DataFrame is then saved as a CSV file named `'train.csv'` using the `to_csv()` method. The `index=False` and `header=False` arguments ensure that the index and header are not written to the CSV file.\n",
    "\n",
    "2. `boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')`: This line uses the `boto3` library to upload the `'train.csv'` file to an Amazon S3 bucket. It creates an S3 resource using a new session with `boto3.Session().resource('s3')`. Then, it specifies the bucket to which the file should be uploaded using `Bucket(bucket_name)`. The `os.path.join(prefix, 'train/train.csv')` constructs the S3 object key by joining the `prefix` (which represents the directory path within the bucket) with the file name. Finally, the `upload_file()` method is called to upload the local file `'train.csv'` to the specified S3 object.\n",
    "\n",
    "3. `s3_input_train = s3_input(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')`: This line creates an S3 input channel for the train data using the `s3_input` class. The `s3_data` argument specifies the S3 location of the data, which is constructed by formatting the bucket name and prefix into the S3 URI `'s3://{}/{}/train'`. The `content_type` argument specifies the type of data, which is set to `'csv'` in this case.\n",
    "\n",
    "Overall, this code snippet saves the train data from a DataFrame to a CSV file, uploads the file to an Amazon S3 bucket, and then creates an S3 input channel for the train data using the `s3_input` class. This process prepares the data for training a machine learning model using Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5bfbd698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "### Saving Train And Test Into Buckets\n",
    "## We start with Train Data\n",
    "import os\n",
    "# Save the train data to a CSV file\n",
    "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)],  axis=1).to_csv('test.csv', index=False, header=False)\n",
    "                                                \n",
    "                                               \n",
    "\n",
    "# Upload the train data to S3 Bucket\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "\n",
    "# whenever we train our model the path data is given to S3 bucket\n",
    "# Create an S3 input channel for the train data\n",
    "s3_input_test = s3_input(s3_data='s3://{}/{}/test'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f5cad",
   "metadata": {},
   "source": [
    "### Building Models Xgboot- Inbuilt Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ff93211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "container=get_image_uri(boto3.Session().region_name,'xgboost',repo_version='1.0-1')\n",
    "# the sagemaker contains the build in algorithims and which are present in the containers or images so we need to pull the container or images in my instance which iam running in the sagemaker  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a73fcbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters={\n",
    "    \"max_depth\":\"5\",\n",
    "    \"eta\":\"0.2\",\n",
    "    \"gamma\":\"4\",\n",
    "    \"min_child_weight\":\"6\",\n",
    "    \"subsample\":\"0.7\",\n",
    "    \"objective\":\"binary:logistic\", # because it binary classification algorithim \n",
    "    \"num_round\":50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd4f53",
   "metadata": {},
   "source": [
    "we should run the hyperparameter tuning in the sagemaker because it takes more time ,so what i have done is that i have executed this whole hyperparameter tuning in my local environment and i took out the information and put it over here ,and i have hyperparameters and all this information is w.r.t XG_boost\n",
    "\n",
    "\n",
    "**\"max_depth\": \"5\":**\n",
    "\n",
    "This hyperparameter controls the maximum depth of each tree in the XGBoost ensemble. It limits the depth of the individual decision trees, helping to prevent overfitting. Setting it to 5 means that each tree can have a maximum depth of 5.\n",
    "\"eta\": \"0.2\":\n",
    "\n",
    "Also known as the learning rate, this hyperparameter controls the step size at each iteration while moving toward a minimum of the loss function. A lower learning rate can make the model's training more robust but may require more iterations.\n",
    "\n",
    "**\"gamma\": \"4\":**\n",
    "\n",
    "Gamma is a regularization parameter that encourages pruning of the tree. It specifies a regularization term that penalizes the complexity of the tree. A higher gamma value encourages more aggressive pruning.\n",
    "\"min_child_weight\": \"6\":\n",
    "\n",
    "This hyperparameter specifies the minimum sum of instance weight (hessian) needed in a child. It can be used to control over-fitting. A higher value makes the algorithm more conservative.\n",
    "\n",
    "**\"subsample\": \"0.7\":**\n",
    "\n",
    "Subsample is a fraction of the training data that is randomly sampled to grow trees during each boosting round. Setting it to 0.7 means that 70% of the training data will be used for each boosting round.\n",
    "\n",
    "**\"objective\": \"binary:logistic\":**\n",
    "\n",
    "This specifies the learning task and the corresponding objective function. In this case, it's set to \"binary:logistic,\" indicating that you're performing binary classification, and XGBoost should use logistic regression as the objective function.\n",
    "\n",
    "**\"num_round\": 50:**\n",
    "\n",
    "This hyperparameter specifies the number of boosting rounds or iterations for the training process. The algorithm will perform 50 iterations to build the ensemble of decision trees.\n",
    "\n",
    "- It's important to note that this code snippet primarily defines hyperparameters and their values. To perform hyperparameter tuning using SageMaker, you would typically create a SageMaker Estimator object, set up a HyperparameterTuner to search for the best combination of hyperparameters, and then start the tuning job. The SageMaker platform would take care of the distributed training and optimization of hyperparameters for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "71664aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_max_run has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_use_spot_instances has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_max_wait has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_volume_size has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "NOTEBOOK_METADATA_FILE detected but failed to get valid domain and user from it.\n"
     ]
    }
   ],
   "source": [
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=container,  # here iam giving the container which i have pulled the image of XG_Boost \n",
    "                                          hyperparameters=hyperparameters, # here i have specified the hyperparameters which are in key-value paris in the dictionaries\n",
    "                                          role=sagemaker.get_execution_role(), # the role aslo need to be specified because the IAMrole invovled in this particular instance so that we can access those S3 buckets to pull the training and test for training purposue\n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.m5.2xlarge', # i have taken as ml.m5.2xlarge because i have perfom the training fast\n",
    "                                          train_volume_size=5, # 5 GB \n",
    "                                          output_path=output_path, # i specifed the output path where the model should be saved and my output path is s3://bankapplication126/Xgboost-as-a-built-in-algo/output\n",
    "                                          train_use_spot_instances=True, # this and rest of the below 2 lines i have used it because it reduce the billing time  as the sagemaker runs based on the running time \n",
    "                                          train_max_run=300,\n",
    "                                          train_max_wait=600)\n",
    "\n",
    "#now i finally i pulled the whole image of the XG_Boost and now iam going to fit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2c150",
   "metadata": {},
   "source": [
    "Sure! Let me explain the code you provided.\n",
    "\n",
    "The code is used to construct a SageMaker estimator that calls the XGBoost container for training a machine learning model.\n",
    "\n",
    "Here is a breakdown of the code:\n",
    "\n",
    "1. `estimator = sagemaker.estimator.Estimator(...)`: This line creates an instance of the `Estimator` class from the SageMaker SDK.\n",
    "\n",
    "2. `image_uri=container`: The `image_uri` parameter specifies the URI of the container image to use for training. In this case, it is the URI of the XGBoost container image.\n",
    "\n",
    "3. `hyperparameters=hyperparameters`: The `hyperparameters` parameter is used to specify the hyperparameters for the XGBoost algorithm. Hyperparameters are passed as key-value pairs in a dictionary.\n",
    "\n",
    "4. `role=sagemaker.get_execution_role()`: The `role` parameter specifies the IAM role that SageMaker will assume to perform tasks on your behalf, such as accessing S3 buckets for data and saving the model artifacts.\n",
    "\n",
    "5. `train_instance_count=1`: The `train_instance_count` parameter specifies the number of instances to use for training. In this case, it is set to 1, meaning training will be performed on a single instance.\n",
    "\n",
    "6. `train_instance_type='ml.m5.2xlarge'`: The `train_instance_type` parameter specifies the type of instance to use for training. In this case, it is set to `ml.m5.2xlarge`, which is a specific instance type optimized for general-purpose machine learning workloads.\n",
    "\n",
    "7. `train_volume_size=5`: The `train_volume_size` parameter specifies the size (in GB) of the EBS volume to use for storing data during training. In this case, it is set to 5 GB.\n",
    "\n",
    "8. `output_path=output_path`: The `output_path` parameter specifies the S3 location where the model artifacts and training output should be saved.\n",
    "\n",
    "9. `train_use_spot_instances=True`: The `train_use_spot_instances` parameter specifies whether to use Amazon EC2 Spot Instances for training. Spot Instances can significantly reduce the cost of training but may be interrupted if the spot price exceeds your bid price.\n",
    "\n",
    "10. `train_max_run=300`: The `train_max_run` parameter specifies the maximum time in seconds that training can run before it is stopped. In this case, it is set to 300 seconds (5 minutes).\n",
    "\n",
    "11. `train_max_wait=600`: The `train_max_wait` parameter specifies the maximum time in seconds that SageMaker will wait for a spot instance to become available. If a spot instance is not available within this time, training will be terminated. In this case, it is set to 600 seconds (10 minutes).\n",
    "\n",
    "That's a high-level explanation of the code you provided. Let me know if you have any further questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c252755b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-10-05-10-04-22-586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-05 10:04:22 Starting - Starting the training job...\n",
      "2023-10-05 10:04:37 Starting - Preparing the instances for training......\n",
      "2023-10-05 10:05:39 Downloading - Downloading input data......\n",
      "2023-10-05 10:06:51 Training - Training image download completed. Training in progress.\n",
      "2023-10-05 10:06:51 Uploading - Uploading generated training model\u001b[34m[2023-10-05 10:06:42.105 ip-10-2-252-161.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[10:06:42] 28831x59 matrix with 1701029 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[10:06:42] 12357x59 matrix with 729063 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2023-10-05 10:06:42.275 ip-10-2-252-161.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-05 10:06:42.276 ip-10-2-252-161.ec2.internal:7 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-05 10:06:42.276 ip-10-2-252-161.ec2.internal:7 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-05 10:06:42.277 ip-10-2-252-161.ec2.internal:7 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-05 10:06:42.277 ip-10-2-252-161.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 28831 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 12357 rows\u001b[0m\n",
      "\u001b[34m[10:06:42] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.10079#011validation-error:0.10528\u001b[0m\n",
      "\u001b[34m[2023-10-05 10:06:42.324 ip-10-2-252-161.ec2.internal:7 INFO hook.py:423] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2023-10-05 10:06:42.327 ip-10-2-252-161.ec2.internal:7 INFO hook.py:486] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.09968#011validation-error:0.10456\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.10017#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.09989#011validation-error:0.10310\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.09996#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.09906#011validation-error:0.10261\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.09930#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.09951#011validation-error:0.10261\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.09920#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.09871#011validation-error:0.10294\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.09868#011validation-error:0.10294\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.09868#011validation-error:0.10326\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.09854#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.09892#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.09850#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.09844#011validation-error:0.10326\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.09857#011validation-error:0.10318\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.09799#011validation-error:0.10318\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.09816#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.09857#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.09830#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.09826#011validation-error:0.10318\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.09847#011validation-error:0.10399\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.09833#011validation-error:0.10407\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.09812#011validation-error:0.10415\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.09812#011validation-error:0.10399\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.09774#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.09781#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.09781#011validation-error:0.10391\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.09778#011validation-error:0.10367\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.09781#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.09771#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.09743#011validation-error:0.10391\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.09753#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.09767#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.09757#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.09757#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.09736#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.09750#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.09733#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.09705#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.09701#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.09712#011validation-error:0.10407\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.09698#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.09733#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.09736#011validation-error:0.10367\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.09746#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.09736#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.09712#011validation-error:0.10334\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.09712#011validation-error:0.10318\u001b[0m\n",
      "\n",
      "2023-10-05 10:07:01 Completed - Training job completed\n",
      "Training seconds: 82\n",
      "Billable seconds: 44\n",
      "Managed Spot Training savings: 46.3%\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train':s3_input_train,'validation':s3_input_test}) # se_input_test is my path of the data which is present in s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b37a49",
   "metadata": {},
   "source": [
    "### Deploy Machine Learning Model As Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cb2f38aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2023-10-05-10-16-45-412\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2023-10-05-10-16-45-412\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2023-10-05-10-16-45-412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge') # initial_instance_count helps you to have parallel instances so that multi parallel processing can happen to get response quickly,and instance type i selected has ml.m4.xlarge which it give very fast response "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ad084",
   "metadata": {},
   "source": [
    "whenever i do load the S3 bucket in that if i click on the bank application >ouput> the model file get created \n",
    "and every when i train my model on new data a new file get created and remember a new file get created based on the time stamps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4ef917",
   "metadata": {},
   "source": [
    "### Prediction of the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "60cbca3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12357,)\n"
     ]
    }
   ],
   "source": [
    "# Convert test_data to CSV format because whenever we are giving data to end points the endpoints will be accepting some input and input is in the form of an Excel_dataset and it given to the model and model will actually give the output\n",
    "test_data_csv = test_data.drop(['y_no', 'y_yes'], axis=1).to_csv(index=False, header=False)\n",
    "\n",
    "# Set the data type for inference\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "\n",
    "# Predict\n",
    "predictions = xgb_predictor.predict(test_data_csv).decode('utf-8') # when we are doing prediction it should be in encoded format so we need to decoded that\n",
    "\n",
    "# Convert predictions to an array\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # now once i get the predictions , i will take the first part of the data so that we will get highest value w.r.t binary classifcation\n",
    "\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8403ea34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05214286, 0.05660191, 0.05096195, ..., 0.03436061, 0.02942475,\n",
       "       0.03715819])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_array # this is the output of my test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "13ef6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Classification Rate: 89.7%\n",
      "\n",
      "Predicted      No Purchase    Purchase\n",
      "Observed\n",
      "No Purchase    91% (10785)    34% (151)\n",
      "Purchase        9% (1124)     66% (297) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix and the whole code is taken from AWS page\n",
    "cm = pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]; fn = cm.iloc[1,0]; tp = cm.iloc[1,1]; fp = cm.iloc[0,1]; p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", p))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"No Purchase\", \"Purchase\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"No Purchase\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Purchase\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1378822b",
   "metadata": {},
   "source": [
    "### observation\n",
    "the accuracy is very very less because it is imbalance dataset\n",
    "and as we can see the right side is actual value and above one is predicted value\n",
    "and we can improve the model performance with help of the hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a1a65",
   "metadata": {},
   "source": [
    "### Imp \n",
    "**And dont run the code again and again once the end points get created  we need to delete those so for deleting use the below code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8c75d6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-xgboost-2023-10-05-10-16-45-412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': '8D0MN301X9PQJFBA',\n",
       "   'HostId': 'OUNhqudG/uhbEYnUOMnUMD0LDawvnwMfBURc3zy1SHj3hiAZAIdmO5QkRq+A4gH6UnyZ3lk76qg=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': 'OUNhqudG/uhbEYnUOMnUMD0LDawvnwMfBURc3zy1SHj3hiAZAIdmO5QkRq+A4gH6UnyZ3lk76qg=',\n",
       "    'x-amz-request-id': '8D0MN301X9PQJFBA',\n",
       "    'date': 'Thu, 05 Oct 2023 10:57:38 GMT',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3',\n",
       "    'connection': 'close'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/output/model.tar.gz'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/events/000000000020/000000000020_worker_0.tfevents'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/index/000000000/000000000000_worker_0.json'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/index/000000000/000000000040_worker_0.json'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/claim.smd'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/profiler-output/system/training_job_end.ts'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/test/test.csv'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/train/train.csv'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/index/000000000/000000000020_worker_0.json'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/index/000000000/000000000010_worker_0.json'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/training_job_end.ts'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/collections/000000000/worker_0_collections.json'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/profiler-output/framework/training_job_end.ts'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/index/000000000/000000000030_worker_0.json'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/events/000000000000/000000000000_worker_0.tfevents'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/events/000000000030/000000000030_worker_0.tfevents'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/events/000000000040/000000000040_worker_0.tfevents'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/debug-output/events/000000000010/000000000010_worker_0.tfevents'},\n",
       "   {'Key': 'Xgboost-as-a-built-in-algo/output/sagemaker-xgboost-2023-10-05-10-04-22-586/profiler-output/system/incremental/2023100510/1696500360.algo-1.json'}]}]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)\n",
    "bucket_to_delete=boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43e11f",
   "metadata": {},
   "source": [
    "### Observation\n",
    "So finally we have completed the deployment of ml model in the Amazon Sagemaker and deleted it and  data which we stored in S3 bucket got deleted in aws services\n",
    "dont run the code top to botton untill and unless if you want to make practice, otherwise they make charges if we run the code continuoslly\n",
    "\n",
    "and if we dont want to get more on the bill just do delete all the files which are present in the notebook instances like ipynb file ,train.csv,test.csv files in order to not to  get more charges in montly billing section of AWS services "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c0bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af7bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b76db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22432a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
